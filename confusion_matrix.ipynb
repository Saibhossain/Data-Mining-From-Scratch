{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/saibhossain/confusion-matrix?scriptVersionId=288756815\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### What is a Confusion Matrix?\n\nA Confusion Matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. [IBM](https://www.ibm.com/think/topics/confusion-matrix)\n\nIt compares Actual Values (Ground Truth) against Predicted Values (What your AI said).\n\nLet's use a Medical Example (relevant to your research):\n\n1. Positive (1): Patient has Cancer.\n\n2. Negative (0): Patient is Healthy.\n\n|                     | **Predicted: NO (Healthy)**                                      | **Predicted: YES (Cancer)**                                             |\n|---------------------|------------------------------------------------------------------|-------------------------------------------------------------------------|\n| **Actual: NO (Healthy)** | **True Negative (TN)**<br>(Correct: Doctor said healthy, patient is healthy) | **False Positive (FP)**<br>(Type I Error: Doctor said cancer, but patient is healthy â€” \"False Alarm\") |\n| **Actual: YES (Cancer)** | **False Negative (FN)**<br>(Type II Error: Doctor said healthy, but patient has cancer â€” \"Missed Diagnosis\") | **True Positive (TP)**<br>(Correct: Doctor said cancer, patient has cancer) |","metadata":{}},{"cell_type":"markdown","source":"# Dummy Data (Ground Truth vs Predictions)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n\n# 1. Define 3 Classes\nclasses = ['Apple', 'Banana', 'Cherry']\n\n# 2. Generate Dummy Data (Ground Truth vs Predictions)\n# 0 = Apple, 1 = Banana, 2 = Cherry\n# We simulate a model that is mostly correct but makes specific errors.\ny_true = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 0, 1, 2]\ny_pred = [0, 0, 1, 0, 1, 1, 2, 1, 2, 2, 0, 2, 0, 1, 2]\n\n# 3. Compute Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# 4. Generate Classification Report (Precision, Recall, F1)\nreport = classification_report(y_true, y_pred, target_names=classes)\n\nprint(\"--- 3-Class Confusion Matrix Data ---\")\nprint(cm)\nprint(\"\\n--- Detailed Classification Report ---\")\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:36:33.948448Z","iopub.execute_input":"2025-12-27T18:36:33.948754Z","iopub.status.idle":"2025-12-27T18:36:36.865873Z","shell.execute_reply.started":"2025-12-27T18:36:33.948727Z","shell.execute_reply":"2025-12-27T18:36:36.864611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Visualize\nplt.figure(figsize=(8, 6))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n\n# Plot with a Green color map\ndisp.plot(cmap=plt.cm.Greens)\nplt.title('3-Class Confusion Matrix: Fruit Classification')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:15:40.770007Z","iopub.execute_input":"2025-12-27T17:15:40.77035Z","iopub.status.idle":"2025-12-27T17:15:40.938901Z","shell.execute_reply.started":"2025-12-27T17:15:40.770321Z","shell.execute_reply":"2025-12-27T17:15:40.938219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# First convert the Multi class matrix in to Binary class matrix","metadata":{}},{"cell_type":"markdown","source":"Standard formulas like Precision, Recall, and F1-Score are mathematically designed for only TWO categories (Positive vs. Negative). They simply cannot handle a 3x3 or 5x5 grid directly.\n\nTo use these formulas on a multi-class problem (like 3 types of lung nodules), we must simplify the complex view into a series of simple \"Yes/No\" questions.\n\n\nIn a 3-Class World (Apple, Banana, Cherry), if the input is an Apple:\n\n* Is \"Banana\" a Negative? Yes.\n\n* Is \"Cherry\" a Negative? Yes.\n\n\nThe formula doesn't care which wrong fruit it is; it only cares that it is NOT an Apple. Therefore, we must group \"Banana\" and \"Cherry\" together into a single \"Not Apple\" bucket to fit the math.\n\n\n|                     | **Predicted A**                          | **Predicted NOT A**                        |\n|---------------------|------------------------------------------|--------------------------------------------|\n| **Actual A**        | **TP** (True Positive)         | **FN** (False Negative) |\n| **Actual NOT A**    | **FP** (False Positive) | **TN** (True Negative) |","metadata":{}},{"cell_type":"code","source":"# Dummy Dataset\n\n# Let's assume: 0 = Apple, 1 = Banana, 2 = Cherry\ny_true = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 0, 1, 2]\ny_pred = [0, 0, 0, 0, 1, 1, 2, 1, 2, 2, 0, 2, 0, 1, 2]\n\n# Map numbers to names for display\nclass_map = {0: 'Apple', 1: 'Banana', 2: 'Cherry'}\nclasses = [0, 1, 2]\n\nprint(\"Total Samples:\", len(y_true))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T19:13:54.542454Z","iopub.execute_input":"2025-12-27T19:13:54.542815Z","iopub.status.idle":"2025-12-27T19:13:54.553152Z","shell.execute_reply.started":"2025-12-27T19:13:54.542784Z","shell.execute_reply":"2025-12-27T19:13:54.552275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper Function Binary Matrix per Class\n\nimport matplotlib.pyplot as plt\n\ndef get_binary_matrix(y_true, y_pred,target_class):\n    TP, TN, FP, FN = 0, 0, 0, 0\n    for i in range(len(y_true)):\n        actual = y_true[i]\n        predicted = y_pred[i]\n\n        if actual == target_class and predicted == target_class:\n            TP += 1 \n        elif actual != target_class and predicted != target_class:\n            TN += 1\n        elif actual != target_class and predicted == target_class:\n            FP += 1\n        elif actual == target_class and predicted != target_class:\n            FN += 1\n\n    return [[TN,FP],[FN,TP]]\n\ndef plot_binary_matrix(matrix,class_name):\n    fig, ax = plt.subplots(figsize=(6,4))\n    im = ax.imshow(matrix,cmap='Blues',vmin=0)\n\n    labels = [['TN', 'FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            ax.text(j, i, f\"{labels[i][j]}\\n{matrix[i][j]}\", \n                    ha='center', va='center', color='black', \n                    fontsize=22, fontweight='bold')\n\n    ax.set_title(f\"Confusion Matrix: {class_name}\", fontsize=14)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xticklabels([f'Not {class_name}', class_name])\n    ax.set_yticklabels([f'Not {class_name}', class_name])\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:49:45.805838Z","iopub.execute_input":"2025-12-27T16:49:45.8067Z","iopub.status.idle":"2025-12-27T16:49:45.815682Z","shell.execute_reply.started":"2025-12-27T16:49:45.806667Z","shell.execute_reply":"2025-12-27T16:49:45.814685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Matrix for Apple class 0\n\napple_matrix = get_binary_matrix(y_true, y_pred, target_class=0)\nTN, FP = apple_matrix[0]\nFN, TP = apple_matrix[1]\nprint(f\"Matrix: \\n [[TN={TN}  FP={FP}] \\n  [FN={FN}   TP={TP}]]\\n\\n\")\n\nplot_binary_matrix(apple_matrix,\"Apple\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:12:08.409216Z","iopub.execute_input":"2025-12-27T17:12:08.410147Z","iopub.status.idle":"2025-12-27T17:12:08.504553Z","shell.execute_reply.started":"2025-12-27T17:12:08.410109Z","shell.execute_reply":"2025-12-27T17:12:08.503775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Matrix for Banana class 1\n\nBanana_matrix = get_binary_matrix(y_true, y_pred, target_class=1)\nTN, FP = Banana_matrix[0]\nFN, TP = Banana_matrix[1]\nprint(f\"Matrix: \\n [[TN={TN}  FP={FP}] \\n  [FN={FN}   TP={TP}]]\\n\\n\")\n\nplot_binary_matrix(Banana_matrix,\"Banana\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:12:11.882183Z","iopub.execute_input":"2025-12-27T17:12:11.882526Z","iopub.status.idle":"2025-12-27T17:12:11.974476Z","shell.execute_reply.started":"2025-12-27T17:12:11.8825Z","shell.execute_reply":"2025-12-27T17:12:11.973703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Matrix for Cherry class 2\n\nCherry_matrix = get_binary_matrix(y_true, y_pred, target_class=2)\nTN, FP = Cherry_matrix[0]\nFN, TP = Cherry_matrix[1]\nprint(f\"Matrix: \\n [[TN={TN}  FP={FP}] \\n  [FN={FN}   TP={TP}]]\\n\\n\")\n\nplot_binary_matrix(Cherry_matrix,\"Cherry\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:18:59.305363Z","iopub.execute_input":"2025-12-27T17:18:59.30629Z","iopub.status.idle":"2025-12-27T17:18:59.401238Z","shell.execute_reply.started":"2025-12-27T17:18:59.306252Z","shell.execute_reply":"2025-12-27T17:18:59.400428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The Metrics: Formulas & Meaning","metadata":{}},{"cell_type":"code","source":"# Helper Function \n\ndef get_counts(y_true, y_pred, target_class):\n    TP, TN, FP, FN = 0, 0, 0, 0\n    for i in range(len(y_true)):\n        if y_true[i] == target_class and y_pred[i] == target_class:\n            TP += 1 \n        elif y_true[i] != target_class and y_pred[i] != target_class:\n            TN += 1\n        elif y_true[i] != target_class and y_pred[i] == target_class:\n            FP += 1\n        elif y_true[i] == target_class and y_pred[i] != target_class:\n            FN += 1\n            \n    print(f\"\\n[Counts for Class {target_class}] TP={TP}, TN={TN}, FP={FP}, FN={FN}\")\n    return TP, TN, FP, FN\n\n\ndef calculate_accuracy(y_true, y_pred, target_class):\n    print(f\"\\n--- Calculating ACCURACY (Class {target_class}) ---\")\n    TP, TN, FP, FN = get_counts(y_true, y_pred, target_class)\n    print(\"Formula: (TP + TN) / (TP + TN + FP + FN)\")\n    print(f\"Values:  ({TP} + {TN}) / ({TP} + {TN} + {FP} + {FN})\")\n    numerator = TP + TN\n    denominator = TP + TN + FP + FN\n    print(f\"Step 1:  {numerator} / {denominator}\")\n    \n    if denominator == 0:\n        print(\"Result:  0.0 (Division by Zero)\")\n        return 0.0\n    else:\n        result = numerator / denominator\n        print(f\"Result:  {result:.4f}\")\n        return result\n\ndef calculate_precision(y_true, y_pred, target_class):\n    print(f\"\\n--- Calculating PRECISION (Class {target_class}) ---\")\n    TP, TN, FP, FN = get_counts(y_true, y_pred, target_class)\n    print(\"Formula: TP / (TP + FP)\")\n    print(f\"Values:  {TP} / ({TP} + {FP})\")\n    numerator = TP\n    denominator = TP + FP\n    print(f\"Step 1:  {numerator} / {denominator}\")\n    \n    if denominator == 0:\n        print(\"Result:  0.0 (Division by Zero)\")\n        return 0.0\n    else:\n        result = numerator / denominator\n        print(f\"Result:  {result:.4f}\")\n        return result\n\ndef calculate_recall(y_true, y_pred, target_class):\n    print(f\"\\n--- Calculating RECALL (Class {target_class}) ---\")\n    TP, TN, FP, FN = get_counts(y_true, y_pred, target_class)\n    print(\"Formula: TP / (TP + FN)\")\n    print(f\"Values:  {TP} / ({TP} + {FN})\")\n\n    numerator = TP\n    denominator = TP + FN\n    print(f\"Step 1:  {numerator} / {denominator}\")\n\n    if denominator == 0:\n        print(\"Result:  0.0 (Division by Zero)\")\n        return 0.0\n    else:\n        result = numerator / denominator\n        print(f\"Result:  {result:.4f}\")\n        return result\n\ndef calculate_f1(y_true, y_pred, target_class):\n    print(f\"\\n--- Calculating F1 SCORE (Class {target_class}) ---\")\n    TP, TN, FP, FN = get_counts(y_true, y_pred, target_class)\n    prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n    rec  = TP / (TP + FN) if (TP + FN) > 0 else 0\n\n    print(\"Formula: 2 * (Precision * Recall) / (Precision + Recall)\")\n    print(f\"Values:  2 * ({prec:.2f} * {rec:.2f}) / ({prec:.2f} + {rec:.2f})\")\n\n    numerator = 2 * (prec * rec)\n    denominator = prec + rec\n    print(f\"Step 1:  {numerator:.4f} / {denominator:.4f}\")\n    \n    if denominator == 0:\n        print(\"Result:  0.0 (Division by Zero)\")\n        return 0.0\n    else:\n        result = numerator / denominator\n        print(f\"Result:  {result:.4f}\")\n        return result\n\n\ndef calculate_specificity(y_true, y_pred, target_class):\n    print(f\"\\n--- Calculating SPECIFICITY (Class {target_class}) ---\")\n    TP, TN, FP, FN = get_counts(y_true, y_pred, target_class)\n    print(\"Formula: TN / (TN + FP)\")\n    print(f\"Values:  {TN} / ({TN} + {FP})\")\n    \n    denominator = TN + FP\n    if denominator == 0:\n        print(\"Result:  0.0 (Division by Zero)\")\n        return 0.0\n        \n    result = TN / denominator\n    print(f\"Result:  {result:.4f}\")\n    return result\n\ndef calculate_error_rate(y_true, y_pred):\n    print(f\"\\n--- Calculating GLOBAL ERROR RATE ---\")\n    \n    incorrect = 0\n    total = len(y_true)\n    \n    for i in range(total):\n        if y_true[i] != y_pred[i]:\n            incorrect += 1\n            \n    print(\"Formula: (Incorrect Predictions) / Total Samples\")\n    print(f\"Values:  {incorrect} / {total}\")\n    \n    if total == 0:\n        return 0.0\n        \n    result = incorrect / total\n    print(f\"Result:  {result:.4f} (or {result*100:.1f}%)\")\n    return result\n\n\ndef calculate_macro_f1(y_true, y_pred):\n    print(f\"\\n=== Calculating MACRO F1 SCORE ===\")\n    print(\"(Step 1: Calculate F1 for each class independently)\")\n    \n    unique_classes = sorted(set(y_true))\n    f1_sum = 0\n    n_classes = len(unique_classes)\n\n    for c in unique_classes:\n        TP, TN, FP, FN = get_counts(y_true, y_pred, c)\n        prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n        rec  = TP / (TP + FN) if (TP + FN) > 0 else 0\n        f1   = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n        \n        print(f\" > Class {c}: F1 = {f1:.4f}\")\n        f1_sum += f1\n        \n    print(\"\\n(Step 2: Average them)\")\n    print(\"Formula: Sum of F1 Scores / Number of Classes\")\n    print(f\"Values:  {f1_sum:.4f} / {n_classes}\")\n    \n    macro_f1 = f1_sum / n_classes\n    print(f\"Result:  {macro_f1:.4f}\")\n    return macro_f1\n\ndef calculate_weighted_f1(y_true, y_pred):\n    print(f\"\\n=== Calculating WEIGHTED F1 SCORE ===\")\n    print(\"(Step 1: Calculate F1 and Support for each class)\")\n    \n    unique_classes = sorted(set(y_true))\n    total_samples = len(y_true)\n    weighted_sum = 0\n    \n    print(f\"{'Class':<6} | {'F1 Score':<10} | {'Support':<8} | {'Weighted Contribution':<20}\")\n    print(\"-\" * 55)\n    \n    for c in unique_classes:\n        TP, TN, FP, FN = get_counts(y_true, y_pred, c)\n        prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n        rec  = TP / (TP + FN) if (TP + FN) > 0 else 0\n        f1   = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n        support = 0\n        for val in y_true:\n            if val == c: support += 1\n        weight_factor = support / total_samples\n        contribution = f1 * weight_factor\n        weighted_sum += contribution\n        \n        print(f\"{c:<6} | {f1:.4f}     | {support:<8} | {f1:.2f} * ({support}/{total_samples}) = {contribution:.4f}\")\n\n    print(\"-\" * 55)\n    print(\"\\n(Step 2: Sum the contributions)\")\n    print(\"Formula: Sum(F1_class * (Support_class / Total_Samples))\")\n    print(f\"Result:  {weighted_sum:.4f}\")\n    return weighted_sum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:37:35.793579Z","iopub.execute_input":"2025-12-27T18:37:35.793968Z","iopub.status.idle":"2025-12-27T18:37:35.817803Z","shell.execute_reply.started":"2025-12-27T18:37:35.793936Z","shell.execute_reply":"2025-12-27T18:37:35.816772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These metrics translate the four confusion matrix components (TP, TN, FP, FN) into interpretable performance scores.\n\n---\n\n### A. Accuracy  \n**Definition**: The percentage of total predictions our model got right.  \n**Formula**:  \n$$\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n$$  \n**Why use it?**  \nGood for general intuition when classes are **balanced** (50% healthy, 50% cancer).  \n\n**âš ï¸ The Trap**:  \nIf 95% of patients are healthy, a model that *always predicts \"Healthy\"* achieves 95% accuracyâ€”but is clinically useless. **Never rely on accuracy alone in medical AI** with imbalanced data.","metadata":{}},{"cell_type":"code","source":"acc = calculate_accuracy(y_true, y_pred, target_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:20:58.760135Z","iopub.execute_input":"2025-12-27T17:20:58.760495Z","iopub.status.idle":"2025-12-27T17:20:58.765632Z","shell.execute_reply.started":"2025-12-27T17:20:58.760465Z","shell.execute_reply":"2025-12-27T17:20:58.764562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = calculate_accuracy(y_true, y_pred, target_class=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:20:46.91752Z","iopub.execute_input":"2025-12-27T17:20:46.918454Z","iopub.status.idle":"2025-12-27T17:20:46.923075Z","shell.execute_reply.started":"2025-12-27T17:20:46.918408Z","shell.execute_reply":"2025-12-27T17:20:46.922068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = calculate_accuracy(y_true, y_pred, target_class=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:21:07.972827Z","iopub.execute_input":"2025-12-27T17:21:07.973175Z","iopub.status.idle":"2025-12-27T17:21:07.978137Z","shell.execute_reply.started":"2025-12-27T17:21:07.973144Z","shell.execute_reply":"2025-12-27T17:21:07.977248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### B. Precision  \n**Definition**: Of all patients predicted as having cancer, how many actually had it?  \n**Formula**:  \n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$  \n**Focus**: *Quality* of positive predictions.  \n**When to use**: When **minimizing False Positives (FP)** is critical.  \n**Example**: Email spam detectionâ€”you donâ€™t want to misclassify an important email as spam (FP).","metadata":{}},{"cell_type":"code","source":"prec = calculate_precision(y_true, y_pred, target_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:21:20.189468Z","iopub.execute_input":"2025-12-27T17:21:20.189824Z","iopub.status.idle":"2025-12-27T17:21:20.194933Z","shell.execute_reply.started":"2025-12-27T17:21:20.189792Z","shell.execute_reply":"2025-12-27T17:21:20.193724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prec = calculate_precision(y_true, y_pred, target_class=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:21:28.358275Z","iopub.execute_input":"2025-12-27T17:21:28.358654Z","iopub.status.idle":"2025-12-27T17:21:28.363345Z","shell.execute_reply.started":"2025-12-27T17:21:28.358625Z","shell.execute_reply":"2025-12-27T17:21:28.362647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prec = calculate_precision(y_true, y_pred, target_class=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:21:37.586948Z","iopub.execute_input":"2025-12-27T17:21:37.587287Z","iopub.status.idle":"2025-12-27T17:21:37.592356Z","shell.execute_reply.started":"2025-12-27T17:21:37.587258Z","shell.execute_reply":"2025-12-27T17:21:37.591447Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### C. Recall (Sensitivity)  \n**Definition**: Of all patients who actually have cancer, how many did the model correctly identify?  \n**Formula**:  \n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$  \n**Focus**: *Coverage* of actual positives.  \n**When to use**: When **minimizing False Negatives (FN)** is vital.  \n**Example**: **Lung cancer screening**â€”itâ€™s safer to trigger extra tests for a healthy patient (FP) than to miss a cancer case (FN). **High Recall is critical here.**","metadata":{}},{"cell_type":"code","source":"rec = calculate_recall(y_true, y_pred, target_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:21:59.310862Z","iopub.execute_input":"2025-12-27T17:21:59.311242Z","iopub.status.idle":"2025-12-27T17:21:59.316122Z","shell.execute_reply.started":"2025-12-27T17:21:59.31121Z","shell.execute_reply":"2025-12-27T17:21:59.315237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rec = calculate_recall(y_true, y_pred, target_class=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:22:07.34996Z","iopub.execute_input":"2025-12-27T17:22:07.350269Z","iopub.status.idle":"2025-12-27T17:22:07.355547Z","shell.execute_reply.started":"2025-12-27T17:22:07.350244Z","shell.execute_reply":"2025-12-27T17:22:07.354588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rec = calculate_recall(y_true, y_pred, target_class=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:22:15.211055Z","iopub.execute_input":"2025-12-27T17:22:15.211498Z","iopub.status.idle":"2025-12-27T17:22:15.216641Z","shell.execute_reply.started":"2025-12-27T17:22:15.211464Z","shell.execute_reply":"2025-12-27T17:22:15.215673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### D. F1 Score  \n**Definition**: The **harmonic mean** of Precision and Recallâ€”balances both.  \n**Formula**:  \n$$\n\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$  \n**When to use**:  \n- When you need a single metric that balances Precision and Recall.  \n- Especially useful for **imbalanced datasets** (e.g., 10 cancer vs. 1000 healthy patients).","metadata":{}},{"cell_type":"code","source":"f1 = calculate_f1(y_true, y_pred, target_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:25:46.38895Z","iopub.execute_input":"2025-12-27T17:25:46.389866Z","iopub.status.idle":"2025-12-27T17:25:46.39493Z","shell.execute_reply.started":"2025-12-27T17:25:46.389831Z","shell.execute_reply":"2025-12-27T17:25:46.393963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1 = calculate_f1(y_true, y_pred, target_class=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:25:54.87654Z","iopub.execute_input":"2025-12-27T17:25:54.876874Z","iopub.status.idle":"2025-12-27T17:25:54.881642Z","shell.execute_reply.started":"2025-12-27T17:25:54.876846Z","shell.execute_reply":"2025-12-27T17:25:54.880725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1 = calculate_f1(y_true, y_pred, target_class=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:26:02.905481Z","iopub.execute_input":"2025-12-27T17:26:02.905804Z","iopub.status.idle":"2025-12-27T17:26:02.910777Z","shell.execute_reply.started":"2025-12-27T17:26:02.905779Z","shell.execute_reply":"2025-12-27T17:26:02.90979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### E. **Macro F1 Score**\n- **Definition**: Unweighted average of F1 scores for each class. Treats all classes equally.\n- **Formula**:  \n  $$\n  \\text{Macro F1} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{F1}_i\n  $$\n  where \\( C \\) = number of classes, and \\( \\text{F1}_i \\) is the F1 score for class \\( i \\).\n- **Use when**: You care **equally about rare and common classes** (e.g., detecting rare cancer subtypes).","metadata":{}},{"cell_type":"code","source":"calculate_macro_f1(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:39:46.019095Z","iopub.execute_input":"2025-12-27T18:39:46.019576Z","iopub.status.idle":"2025-12-27T18:39:46.026961Z","shell.execute_reply.started":"2025-12-27T18:39:46.01954Z","shell.execute_reply":"2025-12-27T18:39:46.026105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### F. **Weighted F1 Score**\n- **Definition**: Average of F1 scores weighted by the number of true samples (support) in each class.\n- **Formula**:  \n  $$\n  \\text{Weighted F1} = \\frac{\\sum_{i=1}^{C} (\\text{F1}_i \\times \\text{support}_i)}{\\sum_{i=1}^{C} \\text{support}_i}\n  $$\n- **Use when**: You want performance to reflect the **class distribution** (e.g., real-world imbalanced medical data).","metadata":{}},{"cell_type":"code","source":"calculate_weighted_f1(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:40:43.442279Z","iopub.execute_input":"2025-12-27T18:40:43.442661Z","iopub.status.idle":"2025-12-27T18:40:43.448734Z","shell.execute_reply.started":"2025-12-27T18:40:43.442632Z","shell.execute_reply":"2025-12-27T18:40:43.448013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n###  Advanced Metrics: Macro vs. Weighted (for Multi-Class)\n\nWhen dealing with **more than two classes** (*Benign*, *Malignant*, *Normal*), we average per-class scores:\n\n#### ðŸ“Œ Macro F1 Score  \n- Compute F1 for each class **independently**, then take the **unweighted average**.  \n- **Use when**: All classes are **equally important**, even rare ones.\n\n#### ðŸ“Œ Weighted F1 Score  \n- Compute F1 for each class, then **weight by support** (number of true instances per class).  \n- **Use when**: You want performance to reflect the **majority class** more (common in real-world skewed data).","metadata":{}},{"cell_type":"markdown","source":"---\n\n### G. **Error Rate**\n- **Definition**: Proportion of incorrect predictions.\n- **Formula**:  \n  $$\n  \\text{Error Rate} = 1 - \\text{Accuracy} = \\frac{FP + FN}{TP + TN + FP + FN}\n  $$\n- **Interpretation**: Lower is better. Often more intuitive than accuracy in high-stakes settings.","metadata":{}},{"cell_type":"code","source":"calculate_error_rate(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:43:13.741502Z","iopub.execute_input":"2025-12-27T18:43:13.742545Z","iopub.status.idle":"2025-12-27T18:43:13.748647Z","shell.execute_reply.started":"2025-12-27T18:43:13.742504Z","shell.execute_reply":"2025-12-27T18:43:13.747893Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### H. **Specificity (True Negative Rate)**\n- **Definition**: Of all truly healthy (negative) patients, how many were correctly identified?\n- **Formula**:  \n  $$\n  \\text{Specificity} = \\frac{TN}{TN + FP}\n  $$\n- **Use when**: Avoiding **false alarms** is important ( unnecessary biopsies).","metadata":{}},{"cell_type":"code","source":"calculate_specificity(y_true, y_pred, target_class=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:44:00.145629Z","iopub.execute_input":"2025-12-27T18:44:00.146383Z","iopub.status.idle":"2025-12-27T18:44:00.152085Z","shell.execute_reply.started":"2025-12-27T18:44:00.146331Z","shell.execute_reply":"2025-12-27T18:44:00.151404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calculate_specificity(y_true, y_pred, target_class=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:44:12.273217Z","iopub.execute_input":"2025-12-27T18:44:12.274037Z","iopub.status.idle":"2025-12-27T18:44:12.2796Z","shell.execute_reply.started":"2025-12-27T18:44:12.274005Z","shell.execute_reply":"2025-12-27T18:44:12.278832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calculate_specificity(y_true, y_pred, target_class=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:44:22.04362Z","iopub.execute_input":"2025-12-27T18:44:22.044281Z","iopub.status.idle":"2025-12-27T18:44:22.050189Z","shell.execute_reply.started":"2025-12-27T18:44:22.04425Z","shell.execute_reply":"2025-12-27T18:44:22.049385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### I. **Balanced Accuracy**\n- **Definition**: Average of recall (sensitivity) and specificity. Handles class imbalance better than raw accuracy.\n- **Formula**:  \n  $$\n  \\text{Balanced Accuracy} = \\frac{\\text{Recall} + \\text{Specificity}}{2}\n  $$\n- **Use when**: You have **imbalanced classes** but still want a single accuracy-like metric.","metadata":{}},{"cell_type":"code","source":"recall = calculate_recall(y_true, y_pred, target_class=0)\nspecificity = calculate_specificity(y_true, y_pred, target_class=0)\n\n\nprint (\"\\n Balance_accuracy (\",recall ,\"+\", specificity,\")/2\")\nBalance_accuracy = (recall+specificity)/2\nprint(\"\\n Balance_accuracy \",Balance_accuracy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:55:13.053166Z","iopub.execute_input":"2025-12-27T18:55:13.053562Z","iopub.status.idle":"2025-12-27T18:55:13.059791Z","shell.execute_reply.started":"2025-12-27T18:55:13.053532Z","shell.execute_reply":"2025-12-27T18:55:13.058609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------\n\n## ðŸ“ˆ ROC Curve (Receiver Operating Characteristic)\n\n- **What it shows**: Trade-off between **True Positive Rate (Recall/Sensitivity)** and **False Positive Rate (1 âˆ’ Specificity)** at various classification thresholds.\n  \n- **Formula**:\n  - TPR = $ \\frac{TP}{TP + FN} $\n  - FPR = $ \\frac{FP}{FP + TN} $\n    \n\n- **Interpretation**:\n  - A curve **closer to the top-left corner** = better model.\n  - **Diagonal line (AUC = 0.5)** = random guessing.\n- **AUC (Area Under Curve)**:\n  - Summarizes overall performance (0.5 = bad, 1.0 = perfect).\n  - **Robust to class imbalance**â€”useful in medical diagnostics (e.g., cancer detection).\n- **When to use**: Evaluating **binary classifiers**, especially when class distribution is skewed or threshold selection matters.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import roc_curve, auc\n\n\nlb = LabelBinarizer()\nlb.fit([0, 1, 2]) # Tell it we have 3 classes\n\ny_true_bin = lb.transform(y_true)\ny_pred_bin = lb.transform(y_pred)\n\nprint(\"\\n--- Shape of Binarized Data ---\")\nprint(y_true_bin.shape) # Should be (15, 3)\n\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10, 8))\ncolors = ['blue', 'green', 'red']\n\nfor i, color in zip(range(n_classes), colors):\n    class_name = class_map[i]\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f'ROC curve of {class_name} (area = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Multi-Class ROC Curve (One-vs-Rest)')\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T19:17:04.402724Z","iopub.execute_input":"2025-12-27T19:17:04.403182Z","iopub.status.idle":"2025-12-27T19:17:04.615731Z","shell.execute_reply.started":"2025-12-27T19:17:04.40315Z","shell.execute_reply":"2025-12-27T19:17:04.614783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### 6. **Support**\n- **Definition**: Number of actual occurrences of a class in the dataset.\n- **Why it matters**: Helps interpret whether a metric ( per-class F1) is based on robust evidence or just a few samples.\n\n> ðŸ’¡ **Tip**: In **scikit-learn**, `classification_report` shows **precision**, **recall**, **F1**, and **support** per classâ€”and includes **macro avg** and **weighted avg** rows by default.","metadata":{}}]}